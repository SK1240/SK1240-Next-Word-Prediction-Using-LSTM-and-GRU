# ğŸ“ Next Word Prediction using LSTM & GRU
A sleek and educational implementation of **next-word prediction** powered by two advanced **Recurrent Neural Network (RNN)** architectures â€” **Long Short-Term Memory (LSTM)** and **Gated Recurrent Unit (GRU)**.

This project combines **deep learning** and **natural language processing (NLP)** to predict the most likely next word in a given phrase.

It includes **Streamlit**-based **web applications** for **real-time text inference**, **pretrained models**, and interactive training notebooks for further experimentation.

## ğŸš€ Project Highlights

| Feature                    | Description                                                                  |
| -------------------------- | ---------------------------------------------------------------------------- |
| ğŸ§© **Dual Models**         | **LSTM** and **GRU** architectures implemented for comparative next-word prediction. |
| ğŸ’» **Interactive Apps**    | **Two Streamlit applications** for hands-on testing of the models.               |
| ğŸ§  **Pretrained Models**   | Ready-to-use `.h5` model files and corresponding `tokenizer.pkl` tokenizer.   |
| ğŸ“˜ **Notebooks Included**  | Full training and retraining workflows for both models.                      |
| ğŸ”„ **End-to-End Pipeline** | Text preprocessing â†’ tokenization â†’ prediction â†’ decoding.                   |

## ğŸ“‚ Repository Structure

```
Next-Word-Prediction-using-LSTM-GRU/
â”œâ”€â”€ app_LSTM.py               # Streamlit app for LSTM-based inference
â”œâ”€â”€ app_GRU.py                # Streamlit app for GRU-based inference
â”œâ”€â”€ model_LSTM.h5             # Pretrained LSTM model
â”œâ”€â”€ model_GRU.h5              # Pretrained GRU model
â”œâ”€â”€ tokenizer.pkl             # Tokenizer mapping words to indices
â”œâ”€â”€ hamlet.txt                # Sample text dataset used for training
â”œâ”€â”€ experiments.ipynb         # LSTM training and experiment notebook
â”œâ”€â”€ experiments_GRU.ipynb     # GRU training and experiment notebook
â””â”€â”€ requirements.txt          # List of required dependencies
```

## âš™ï¸ Setup & Installation

1ï¸âƒ£ Clone the Repository
```
git clone https://github.com/SK1240/Next-Word-Prediction-Using-LSTM-and-GRU.git
cd Next-Word-Prediction-using-GRU-LSTM
```

2ï¸âƒ£ Create and Activate a Virtual Environment
```
python -m venv .venv
```
Activate the environment:

* Windows: `.venv\Scripts\activate`

* macOS/Linux: `source .venv/bin/activate`

3ï¸âƒ£ Install Dependencies
```
pip install -r requirements.txt
```
ğŸ’¡ If you donâ€™t have GPU support, use `tensorflow-cpu` instead of `tensorflow`.

### â–¶ï¸ How to Run the Apps

Launch the LSTM-based application:
```
streamlit run app_LSTM.py
```

Launch the GRU-based application:
```
streamlit run app_GRU.py
```
Once executed, Streamlit will start a local server (default: [localhost:8501](http://localhost:8501))

Type a short phrase in the text box, click â€œ**Predict Next Word**â€, and view the **modelâ€™s** generated suggestion instantly!

## âš¡ Behind the Scenes

Each app follows a streamlined prediction workflow:

* **Load Tokenizer** â†’ Load `tokenizer.pkl` (used during training).

* **Preprocess Input** â†’ Convert text to token indices.

* **Pad Sequences** â†’ Adjust input to modelâ€™s expected length.

* **Model Inference** â†’ Predict next token using the trained model.

* **Decode Prediction** â†’ Convert predicted index back to its corresponding word.

## ğŸ’¡ Usage Notes

* The **tokenizer** and **model** must belong to the same training session.

* Provide meaningful context (`2â€“5 words`) for accurate predictions.

* To fine-tune or retrain, open the training notebooks, modify parameters or **text corpus**, and re-save the updated model (`.h5`) and tokenizer (`tokenizer.pkl`).


## ğŸ§ª Retraining Process

Each notebook (`experiments.ipynb` and `experiments_GRU.ipynb`) demonstrates:

* **Data Preparation** â€“ Load and clean the text corpus.

* **Tokenization & Sequence Generation** â€“ Map words to integers.

* **Model Construction** â€“ Build **LSTM/GRU** layers using **Keras**.

* **Training & Evaluation** â€“ **Optimize** using **categorical cross-entropy**.

* **Model Saving** â€“ Export trained `.h5` model and `tokenizer.pkl`.

| Notebook                   | Purpose                                |
| -------------------------- | -------------------------------------- |
| ğŸ§© `experiments.ipynb`     | Training and testing of the **LSTM model** |
| âš™ï¸ `experiments_GRU.ipynb` | Training and testing of the **GRU model**  |


## ğŸ Summary

This project demonstrates how sequence modeling and neural text generation can be practically implemented using **LSTM** and **GRU** networks.

With minimal setup and intuitive UI, it serves as a foundation for building more advanced language models and predictive NLP applications.

## ğŸ“œ License

This repository is released for **educational** and **research purposes**.

Users are encouraged to **test**, **modify**, and extend the project responsibly before any production-level deployment.
